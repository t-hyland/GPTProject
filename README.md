# GPTProject
This is a GPT (Generative Pre-Trained Transformer) ML model project. The objective is the reproduce language in the style of the input data.

The first part of this notebook sets up the architecture of the transformer.
The second part trains the model on the plays of Shakespeare and generates new text with the starting characters 'Brother'.
The final part trains the model on a Harry Potter book and generates new text starting with 'Wingardium'.

This notebook takes ~30 minutes to run when connected to a default T4 runtime in Google Colab. The file 'preRan.pdf' showcases a pre-ran, static version of the notebook as a sample. 

Author - Thomas Hyland
