{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This is a GPT (Generative Pre-Trained Transformer) ML model project. The objective is the reproduce language in the style of the input data.  \n",
        "\n",
        "1.   The first part of this notebook sets up the architecture of the transformer.\n",
        "2.   The second part trains the model on the plays of Shakespeare and generates new text with the starting characters 'Brother'.\n",
        "3.   The final part trains the model on a Harry Potter book and generates new text starting with 'Wingardium'.\n",
        "\n",
        "This notebook takes around 30 minutes to run when connected to the default T4 environment. Refer to my github page: https://github.com/t-hyland/GPTProject for a pre-ran, static example of this notebook.\n",
        "\\\n",
        "Author - Thomas Hyland"
      ],
      "metadata": {
        "id": "VYqASlKXhSf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "EXspyciwR-8z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION"
      ],
      "metadata": {
        "id": "XKLs5jsr-uSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the tiny shakespeare dataset\n",
        "if not os.path.exists('data'):\n",
        "  os.makedirs('data')\n",
        "if not os.path.exists('data/shakespeare'):\n",
        "  os.makedirs('data/shakespeare')\n",
        "data_root = 'data/shakespeare'\n",
        "input_file_path = os.path.join(data_root, 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/learn2phoenix/CMSC472_HW6/main/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(data_root, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(data_root, 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(f'{data_root}/meta.pkl', 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ],
      "metadata": {
        "id": "h0SfnEVkiV04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d672295e-c904-4431-b983-cd8b1906b403"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,395\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,855 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path"
      ],
      "metadata": {
        "id": "zfW-HWLs0FxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f11d1ae1-bad7-4871-ea71-f008824c8f31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/shakespeare/input.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n",
        "def new_gelu(x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "5CrWG-UEWXRg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL ARCHITECTURE DEFINITION"
      ],
      "metadata": {
        "id": "NqK0wyxEggcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Casual Self Attention Module Definition:"
      ],
      "metadata": {
        "id": "GCdOiYxpQWCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.config = config\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ],
      "metadata": {
        "id": "SxQvYKPQWZSo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some other utility blocks are defined as:"
      ],
      "metadata": {
        "id": "uRoKHgUHkJa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = new_gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ],
      "metadata": {
        "id": "4CKrJM5MWdyA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Model Architecture Definition:"
      ],
      "metadata": {
        "id": "Zh1NjhlDgrLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        remember to subtract the position embeddings for non_embedding\n",
        "        The token embeddings would have received the same treatement too, but\n",
        "        for their use as weights, due to parameter sharing, in the final layer.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        token_embeddings = self.transformer.wte(idx)\n",
        "        position_ids = torch.arange(idx.size(1), device=idx.device).unsqueeze(0)\n",
        "        position_embeddings = self.transformer.wpe(position_ids)\n",
        "\n",
        "        # Combine token and position embeddings\n",
        "        x = token_embeddings + position_embeddings  # Shape: [batch_size, seq_len, n_embd]\n",
        "        x = self.transformer.drop(x)  # Apply dropout\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)  # Each block processes [batch_size, seq_len, n_embd]\n",
        "        x = self.transformer.ln_f(x)  # Shape: [batch_size, seq_len, n_embd]\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {}\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 1024\n",
        "        config_args['bias'] = True\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        decay.remove('lm_head.weight')\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        flops_achieved = flops_per_iter * (1.0/dt)\n",
        "        flops_promised = 312e12\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "x0nmLFiFnKLN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAKESPEARE TRAINING AND GENERATION"
      ],
      "metadata": {
        "id": "FC8pX51nmXad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN CONFIG\n",
        "out_dir = 'out-shakespeare-char'\n",
        "eval_interval = 250\n",
        "log_interval = 10\n",
        "eval_iters = 200\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "# data\n",
        "dataset = 'shakespeare'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "# model\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.05\n",
        "bias =  False\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 2000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 100\n",
        "lr_decay_iters = 2000\n",
        "min_lr = 1e-4\n",
        "# system\n",
        "device = 'cuda'\n",
        "dtype = 'float16'\n",
        "compile = False\n",
        "\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n"
      ],
      "metadata": {
        "id": "jZoStRV_mKNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4901b8-b4f6-4304-e608-f20a46e6bb7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "QonpBseImY7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "\n",
        "    # Linear warmup phase\n",
        "    if it < warmup_iters:\n",
        "        coefficient = it / warmup_iters\n",
        "    # Cosine decay phase\n",
        "    else:\n",
        "        decay = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "        decay = min(1, max(0, decay))  # Ensure decay is in [0, 1]\n",
        "        assert 0 <= decay <= 1\n",
        "        coefficient = 0.5 * (1 + math.cos(math.pi * decay))  # Cosine decay\n",
        "\n",
        "    return min_lr + coefficient * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "UQb2FyyaR68U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)\n",
        "model_args['vocab_size'] = meta_vocab_size\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model, backend='triton') # requires PyTorch 2.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model\n",
        "running_mfu = -1.0\n",
        "for iter_num in range(max_iters):\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        X, Y = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "    if grad_clip != 0.0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    local_iter_num += 1"
      ],
      "metadata": {
        "id": "EnoNsDhOiMi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99842999-a059-4377-eaff-eb17e8678c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found vocab_size = 65 (inside data/shakespeare/meta.pkl)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.65M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3463420296.py:45: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Text Generation"
      ],
      "metadata": {
        "id": "pLll_WMoma-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "start = \"Brother\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "model.eval()\n",
        "\n",
        "meta_path = 'data/shakespeare/meta.pkl'\n",
        "print(f\"Loading meta from meta.pkl...\")\n",
        "with open(meta_path, 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta['stoi'], meta['itos']\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
      ],
      "metadata": {
        "id": "-BY0EmCMmRcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            sample = model.generate(x, max_new_tokens,\n",
        "                temperature= temperature,\n",
        "                top_k= top_k\n",
        "            )\n",
        "            #print(sample)\n",
        "            sample = np.array(sample.cpu())\n",
        "            #print(decode(sample[0]))\n",
        "            text = decode(sample[0])\n",
        "            print(f'===Sample {k+1}:===\\n', text)"
      ],
      "metadata": {
        "id": "nUHUenqJVkDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HARRY POTTER TRAINING AND GENERATION"
      ],
      "metadata": {
        "id": "4qvKninXYpdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "mQqiqq29RdQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data():\n",
        "  url = 'https://raw.githubusercontent.com/t-hyland/hp4/refs/heads/main/hp4'\n",
        "  if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "  if not os.path.exists('data/harrypotter'):\n",
        "    os.makedirs('data/harrypotter')\n",
        "  data_root = 'data/harrypotter'\n",
        "  input_file_path = os.path.join(data_root, 'input.txt')\n",
        "  if not os.path.exists(input_file_path):\n",
        "    with open(input_file_path, 'w') as f:\n",
        "      f.write(requests.get(url).text)\n",
        "\n",
        "\n",
        "  with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "  print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "  # get all the unique characters that occur in this text\n",
        "  chars = sorted(list(set(data)))\n",
        "  vocab_size = len(chars)\n",
        "  print(\"all the unique characters:\", ''.join(chars))\n",
        "  print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "  # create a mapping from characters to integers\n",
        "  stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "  itos = { i:ch for i,ch in enumerate(chars) }\n",
        "  def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "  def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "  # create the train and test splits\n",
        "  n = len(data)\n",
        "  train_data = data[:int(n*0.9)]\n",
        "  val_data = data[int(n*0.9):]\n",
        "\n",
        "  # encode both to integers\n",
        "  train_ids = encode(train_data)\n",
        "  val_ids = encode(val_data)\n",
        "  print(f\"train has {len(train_ids):,} tokens\")\n",
        "  print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "  # export to bin files\n",
        "  train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "  val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "  train_ids.tofile(os.path.join(data_root, 'train.bin'))\n",
        "  val_ids.tofile(os.path.join(data_root, 'val.bin'))\n",
        "\n",
        "  # save the meta information as well, to help us encode/decode later\n",
        "  meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "  }\n",
        "  with open(f'{data_root}/meta.pkl', 'wb') as f:\n",
        "      pickle.dump(meta, f)\n"
      ],
      "metadata": {
        "id": "WqEIp3WOnE_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data()"
      ],
      "metadata": {
        "id": "2Y0bEtPIo-WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "vL7nVnW3RiW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "  ## TRAIN CONFIG\n",
        "  out_dir = 'out-harrypotter-char'\n",
        "  eval_interval = 250\n",
        "  log_interval = 10\n",
        "  eval_iters = 200\n",
        "  eval_only = False\n",
        "  always_save_checkpoint = False\n",
        "  # data\n",
        "  dataset = 'harrypotter'\n",
        "  gradient_accumulation_steps = 1\n",
        "  batch_size = 64\n",
        "  block_size = 256\n",
        "  # model\n",
        "  n_layer = 6\n",
        "  n_head = 6\n",
        "  n_embd = 384\n",
        "  dropout = 0.05\n",
        "  bias =  False\n",
        "  # adamw optimizer\n",
        "  learning_rate = 1e-3\n",
        "  max_iters = 2000\n",
        "  weight_decay = 1e-1\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.99\n",
        "  grad_clip = 1.0\n",
        "  decay_lr = True\n",
        "  warmup_iters = 100\n",
        "  lr_decay_iters = 2000\n",
        "  min_lr = 1e-4\n",
        "  # system\n",
        "  device = 'cuda'\n",
        "  dtype = 'float16'\n",
        "  compile = False\n",
        "\n",
        "  seed_offset = 0\n",
        "  ddp_world_size = 1\n",
        "  tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "  print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "  def get_lr(it):\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "\n",
        "    # Linear warmup phase\n",
        "    if it < warmup_iters:\n",
        "        coefficient = it / warmup_iters\n",
        "    # Cosine decay phase\n",
        "    else:\n",
        "        decay = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "        decay = min(1, max(0, decay))  # Ensure decay is in [0, 1]\n",
        "        assert 0 <= decay <= 1\n",
        "        coefficient = 0.5 * (1 + math.cos(math.pi * decay))  # Cosine decay\n",
        "    return min_lr + coefficient * (learning_rate - min_lr)\n",
        "\n",
        "  os.makedirs(out_dir, exist_ok=True)\n",
        "  torch.manual_seed(1337 + seed_offset)\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "  ptdtype = {'float32': torch.float32, 'float16': torch.float16}[dtype]\n",
        "  ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "  data_dir = os.path.join('data', dataset)\n",
        "  train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "  val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "  def get_batch(split):\n",
        "      data = train_data if split == 'train' else val_data\n",
        "      ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "      x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "      y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "      if device_type == 'cuda':\n",
        "          x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "      else:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "      return x, y\n",
        "\n",
        "  iter_num = 0\n",
        "  best_val_loss = 1e9\n",
        "\n",
        "  meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "  meta_vocab_size = None\n",
        "  if os.path.exists(meta_path):\n",
        "      with open(meta_path, 'rb') as f:\n",
        "          meta = pickle.load(f)\n",
        "      meta_vocab_size = meta['vocab_size']\n",
        "      print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "  # model init\n",
        "  model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                    bias=bias, vocab_size=None, dropout=dropout)\n",
        "  model_args['vocab_size'] = meta_vocab_size\n",
        "  gptconf = GPTConfig(**model_args)\n",
        "  model = GPT(gptconf)\n",
        "  if block_size < model.config.block_size:\n",
        "      model.crop_block_size(block_size)\n",
        "      model_args['block_size'] = block_size\n",
        "  model.to(device)\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "  checkpoint = None\n",
        "\n",
        "  # compile the model\n",
        "  if compile:\n",
        "      print(\"compiling the model... (takes a ~minute)\")\n",
        "      unoptimized_model = model\n",
        "      model = torch.compile(model, backend='triton') # requires PyTorch 2.0\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss():\n",
        "      out = {}\n",
        "      model.eval()\n",
        "      for split in ['train', 'val']:\n",
        "          losses = torch.zeros(eval_iters)\n",
        "          for k in range(eval_iters):\n",
        "              X, Y = get_batch(split)\n",
        "              with ctx:\n",
        "                  logits, loss = model(X, Y)\n",
        "              losses[k] = loss.item()\n",
        "          out[split] = losses.mean()\n",
        "      model.train()\n",
        "      return out\n",
        "\n",
        "  # training loop\n",
        "  X, Y = get_batch('train') # fetch the very first batch\n",
        "  t0 = time.time()\n",
        "  local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "  raw_model = model\n",
        "  running_mfu = -1.0\n",
        "  for iter_num in range(max_iters):\n",
        "      lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = lr\n",
        "\n",
        "      if iter_num % eval_interval == 0:\n",
        "          losses = estimate_loss()\n",
        "          print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "          if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "              best_val_loss = losses['val']\n",
        "              if iter_num > 0:\n",
        "                  checkpoint = {\n",
        "                      'model': raw_model.state_dict(),\n",
        "                      'optimizer': optimizer.state_dict(),\n",
        "                      'model_args': model_args,\n",
        "                      'iter_num': iter_num,\n",
        "                      'best_val_loss': best_val_loss\n",
        "                  }\n",
        "                  print(f\"saving checkpoint to {out_dir}\")\n",
        "                  torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "      if iter_num == 0 and eval_only:\n",
        "          break\n",
        "\n",
        "      for micro_step in range(gradient_accumulation_steps):\n",
        "          with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "              loss = loss / gradient_accumulation_steps\n",
        "          X, Y = get_batch('train')\n",
        "          scaler.scale(loss).backward()\n",
        "      if grad_clip != 0.0:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      # timing and logging\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "      if iter_num % log_interval == 0:\n",
        "          lossf = loss.item() * gradient_accumulation_steps\n",
        "          if local_iter_num >= 5: # let the training loop settle a bit\n",
        "              mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "              running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "          print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "      local_iter_num += 1"
      ],
      "metadata": {
        "id": "vK_cm_O843s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model()"
      ],
      "metadata": {
        "id": "AzZso_tzqMCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Text Generation"
      ],
      "metadata": {
        "id": "KY60NsDJRuBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples():\n",
        "  # -----------------------------------------------------------------------------\n",
        "  start = \"Wingardium \" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "  num_samples = 10 # number of samples to draw\n",
        "  max_new_tokens = 500 # number of tokens generated in each sample\n",
        "  temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "  top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "  # -----------------------------------------------------------------------------\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  meta_path = 'data/harrypotter/meta.pkl'\n",
        "  print(f\"Loading meta from meta.pkl...\")\n",
        "  with open(meta_path, 'rb') as f:\n",
        "      meta = pickle.load(f)\n",
        "  stoi, itos = meta['stoi'], meta['itos']\n",
        "  encode = lambda s: [stoi[c] for c in s]\n",
        "  decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "  # encode the beginning of the prompt\n",
        "  if start.startswith('FILE:'):\n",
        "      with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "          start = f.read()\n",
        "  start_ids = encode(start)\n",
        "  x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "  # run generation\n",
        "  with torch.no_grad():\n",
        "      with ctx:\n",
        "          for k in range(num_samples):\n",
        "              sample = model.generate(x, max_new_tokens,\n",
        "                  temperature= temperature,\n",
        "                  top_k= top_k\n",
        "              )\n",
        "              #print(sample)\n",
        "              sample = np.array(sample.cpu())\n",
        "              #print(decode(sample[0]))\n",
        "              text = decode(sample[0])\n",
        "              print(f'===Sample {k+1}:===\\n', text)"
      ],
      "metadata": {
        "id": "8iLTEZ0H5BgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_samples()"
      ],
      "metadata": {
        "id": "2p4y2osaptbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}